# üöÄ SignLink - Complete Technology Stack

## üìã Project Overview
**SignLink** is an AI-powered American Sign Language (ASL) translator with real-time gesture recognition, video generation, classroom features, and Chrome extension support.

---

## üéØ Backend Stack

### Core Framework
- **FastAPI** `0.104.1` - Modern async web framework
- **Uvicorn** `0.24.0` - ASGI server with WebSocket support
- **Python** `3.11` - Programming language

### Machine Learning & Computer Vision
- **MediaPipe** `0.10.7` - Hand landmark detection and tracking
- **OpenCV** `4.8.1.78` (opencv-python-headless) - Image processing
- **scikit-learn** `1.3.2` - ML model training and inference
- **PyTorch** `2.1.0+cpu` - Deep learning framework (CPU version)
- **joblib** `1.3.2` - Model serialization/deserialization
- **NumPy** `1.26.4` - Numerical computations

### AI/LLM Integration
- **OpenAI API** `1.3.5` - GPT-4o-mini (text-to-gloss conversion) & Whisper (speech-to-text)
  - Legacy API methods: `openai.Audio.transcribe()`, `openai.ChatCompletion.create()`
  - Models: `gpt-4o-mini`, `whisper-1`

### Video Processing
- **WLASL Dataset v0.3** - 2000+ ASL sign videos
- **MoviePy** / **FFmpeg** - Video composition and editing
- **requests** `2.31.0` - HTTP client for downloading videos

### Data Processing
- **python-multipart** `0.0.6` - File upload handling
- **Pillow** `10.1.0` - Image manipulation
- **aiofiles** `23.2.1` - Async file operations

### Development Tools
- **python-dotenv** `1.0.0` - Environment variable management
- **Jinja2** `3.1.2` - Template engine (optional frontend serving)

---

## üé® Frontend Stack

### Core Framework
- **Node.js** + **Express.js** `4.18.2` - Static file server
- **Vanilla JavaScript (ES6+)** - Client-side logic
- **HTML5** - Markup
- **CSS3** - Styling with custom themes

### Real-time Communication
- **Native WebSocket API** - Classroom teacher-student connections
- **Fetch API** - REST API communication

### Media Handling
- **MediaRecorder API** - Audio recording for transcription
- **Canvas API** - Video frame capture for inference
- **Video API** - Playback of generated ASL videos

### UI Components
- **Custom CSS** - Responsive design
  - `style.css` - Main styles
  - `classroom.css` - Classroom interface
  - `dropdown-fix.css` - UI fixes
  - `enhanced_dropdowns.css` - Select styling
  - `quick_actions.css` - Button groups
  - `voice_controls.css` - Audio controls

---

## üß© Chrome Extension Stack

### Core Technology
- **Manifest V3** - Latest Chrome extension standard
- **JavaScript (ES6+)** - Extension logic
- **Chrome APIs**:
  - `chrome.storage` - Settings persistence
  - `chrome.runtime` - Background communication

### Content Scripts
- **content.js** - YouTube caption interception
- **DOM Manipulation** - Overlay injection
- **Fetch API** - Backend communication

### Features
- YouTube caption capture
- Text tokenization via backend API (`/tokenize-text`)
- Video overlay (`/token-video/{token}`)
- Real-time ASL translation display

---

## üóÑÔ∏è Data & Models

### Pre-trained Models
- **gesture_model.pkl** (11.9 MB) - ASL gesture recognition
- **letter_model.pkl** (11.9 MB) - Fingerspelling letter detection
- **label_map.npy** / **label_map2.npy** - Class label mappings

### Datasets
- **WLASL_v0.3.json** (12.3 MB) - 2000+ ASL signs with video URLs
  - Sources: ASL Bricks, ASL LEX, YouTube
  - Metadata: gloss, video URLs, bbox coordinates

### Model Architecture
- **Scikit-learn Pipeline**:
  - StandardScaler (feature normalization)
  - SVC (Support Vector Classifier) with RBF kernel
  - Input: MediaPipe hand landmarks (63 features: 21 landmarks √ó 3 coordinates)
  - Output: Gesture class (26 letters + 10+ gestures)

---

## üê≥ Deployment Stack

### Backend Deployment (Hugging Face Spaces)
- **Platform**: Hugging Face Spaces
- **Container**: Docker (SDK mode)
- **Base Image**: `python:3.10-slim`
- **Port**: 8000
- **Storage**: Ephemeral `/tmp` storage
  - `/tmp/signlink_outputs/` - Generated videos
  - `/tmp/signlink_videos/` - Cached WLASL videos
- **Environment Variables**:
  - `OPENAI_API_KEY` - OpenAI API authentication
- **Git LFS**: Tracking large files (models, WLASL JSON)

### Frontend Deployment (Planned)
- **Platform**: Vercel
- **Framework**: Node.js + Express
- **Build**: Static file serving
- **Environment**: Production API URL configuration

### Chrome Extension Distribution
- **Platform**: Chrome Web Store
- **Packaging**: Zip with manifest.json
- **Updates**: Manual version bumps

---

## üîó API Architecture

### REST Endpoints
```
GET  /health                          - Health check
GET  /model-status                    - ML model status
POST /infer-frame                     - Gesture detection from image
POST /infer-letter                    - Letter detection from image
POST /reverse-translate-video         - Text ‚Üí ASL video generation
POST /process-confirmed-words         - Gloss ‚Üí English conversion
POST /tokenize-text                   - Text ‚Üí gloss tokens (extension)
GET  /token-video/{token}             - Get ASL video for token
GET  /outputs/{filename}              - Serve generated files
GET  /docs                            - Swagger UI
```

### WebSocket Endpoints
```
WS /ws/classroom/{room_id}/teacher    - Teacher dashboard
WS /ws/classroom/{room_id}/student    - Student view
```

### Communication Flow
```
Frontend (Port 3000)
    ‚Üì Fetch/WebSocket
Backend API (Port 8000)
    ‚Üì OpenAI API
OpenAI GPT-4o-mini / Whisper
    ‚Üì HTTP
WLASL Videos (External CDN)
```

---

## üîê Security & Configuration

### CORS Policy
```python
CORSMiddleware(
    allow_origins=["*"],      # Open for frontend/extension
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)
```

### API Keys
- **OpenAI API Key**: Stored in environment variable `OPENAI_API_KEY`
- **No authentication** on API endpoints (open access)

### File Upload Limits
- **Max file size**: Configurable via FastAPI
- **Allowed MIME types**: `image/jpeg`, `image/png`, `audio/webm`

---

## üì¶ Package Management

### Backend (requirements.txt)
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
mediapipe==0.10.7
opencv-python-headless==4.8.1.78
scikit-learn==1.3.2
torch==2.1.0
joblib==1.3.2
numpy==1.26.4
openai==1.3.5
python-multipart==0.0.6
aiofiles==23.2.1
python-dotenv==1.0.0
requests==2.31.0
pillow==10.1.0
jinja2==3.1.2
```

### Frontend (package.json)
```json
{
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  }
}
```

---

## üõ†Ô∏è Development Tools

### Version Control
- **Git** - Source control
- **Git LFS** - Large file storage (models, datasets)
- **GitHub** - Code hosting (development)
- **Hugging Face Hub** - Deployment repository

### IDE & Extensions
- **VS Code** - Primary editor
- **Python Extension** - Python language support
- **Pylance** - Python language server

### Testing
- **Manual Testing** - Browser-based API testing
- **Swagger UI** - Interactive API documentation at `/docs`
- **Browser DevTools** - Frontend debugging

### Terminal
- **PowerShell** - Windows shell
- **Python Virtual Environment** - `.venv` for isolation

---

## üìä System Requirements

### Development
- **OS**: Windows 10/11 (tested), Linux, macOS
- **RAM**: 8GB minimum (16GB recommended)
- **Storage**: 2GB for dependencies + models
- **Python**: 3.10 or 3.11
- **Node.js**: 16+ (for frontend)

### Production (HF Spaces)
- **CPU**: 2 cores
- **RAM**: 16GB
- **Storage**: Ephemeral (no persistent disk)
- **Network**: Internet access for WLASL video downloads

---

## üéØ Key Features by Component

### Backend Features
‚úÖ Real-time hand gesture recognition (30+ FPS)
‚úÖ Fingerspelling letter detection (A-Z)
‚úÖ Text-to-ASL video generation
‚úÖ Audio transcription (Whisper)
‚úÖ LLM-powered gloss conversion (GPT-4o-mini)
‚úÖ Dynamic WLASL video fetching
‚úÖ WebSocket classroom streaming
‚úÖ HTTP Range support for video streaming

### Frontend Features
‚úÖ Webcam integration for live inference
‚úÖ Audio recording for transcription
‚úÖ Video playback controls
‚úÖ Real-time WebSocket updates
‚úÖ Responsive design (mobile-friendly)
‚úÖ Classroom mode (teacher/student views)
‚úÖ Learn page with example videos

### Chrome Extension Features
‚úÖ YouTube caption interception
‚úÖ Real-time ASL translation overlay
‚úÖ Token-based video fetching
‚úÖ Configurable backend URL
‚úÖ Persistent settings storage

---

## üìà Performance Metrics

### Inference Speed
- **Gesture Recognition**: ~50ms per frame
- **Letter Detection**: ~30ms per frame
- **Video Generation**: 2-5 seconds for 3-word sentence

### Video Cache
- **First Request**: 1-3 seconds (download from WLASL)
- **Cached Request**: <100ms (serve from `/tmp`)
- **Cache Lifetime**: Until Space restart

### API Response Times
- **Health Check**: <10ms
- **Model Status**: <20ms
- **Transcription**: 1-3 seconds (OpenAI Whisper)
- **Gloss Conversion**: 500ms-2s (GPT-4o-mini)

---

## üöÄ Deployment URLs

### Production
- **Backend API**: `https://lamaq-signlink-hackx.hf.space`
- **API Docs**: `https://lamaq-signlink-hackx.hf.space/docs`
- **Health Check**: `https://lamaq-signlink-hackx.hf.space/health`

### Development
- **Backend**: `http://localhost:8000`
- **Frontend**: `http://localhost:3000`
- **API Docs**: `http://localhost:8000/docs`

### Repository
- **HF Space**: `https://huggingface.co/spaces/Lamaq/signlink-hackx`
- **Extension**: Chrome Web Store (pending)

---

## üìù File Structure

```
SignLink/
‚îú‚îÄ‚îÄ backend/                    # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py            # Main FastAPI application (991 lines)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ revtrans.py        # LLM gloss conversion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dynamic_video_fetcher.py  # WLASL video fetcher
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup.py           # Helper functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ boxes.py           # Bounding box utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py          # Logging configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json        # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ detr_model.py      # DETR model definition
‚îÇ   ‚îú‚îÄ‚îÄ pretrained/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gesture_model.pkl  # Gesture recognition model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ letter_model.pkl   # Letter detection model
‚îÇ   ‚îú‚îÄ‚îÄ mapper/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ WLASL_v0.3.json    # ASL video dataset (2000+ signs)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile             # Docker configuration
‚îÇ   ‚îú‚îÄ‚îÄ start.py               # Entry point
‚îÇ   ‚îî‚îÄ‚îÄ README.md              # HF Spaces documentation
‚îÇ
‚îú‚îÄ‚îÄ frontend/                   # Express.js frontend
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html         # Main page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learn.html         # Learning page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ teacher.html       # Teacher dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ student.html       # Student view
‚îÇ   ‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ script.js          # Main JavaScript (1877 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ style.css          # Main styles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classroom.css      # Classroom styles
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.css              # Additional styles
‚îÇ   ‚îú‚îÄ‚îÄ server.js              # Express server
‚îÇ   ‚îî‚îÄ‚îÄ package.json           # Node dependencies
‚îÇ
‚îú‚îÄ‚îÄ chrome_extension/           # Chrome extension
‚îÇ   ‚îú‚îÄ‚îÄ manifest.json          # Extension configuration
‚îÇ   ‚îú‚îÄ‚îÄ content.js             # Content script (YouTube injection)
‚îÇ   ‚îú‚îÄ‚îÄ background.js          # Background service worker
‚îÇ   ‚îú‚îÄ‚îÄ popup.html             # Extension popup
‚îÇ   ‚îî‚îÄ‚îÄ popup.js               # Popup logic
‚îÇ
‚îî‚îÄ‚îÄ signlink-hackx/            # HF Spaces deployment (git repo)
    ‚îî‚îÄ‚îÄ [Same as backend/ structure]
```

---

## üéì Learning Resources

### APIs Used
- **MediaPipe Hands**: https://google.github.io/mediapipe/solutions/hands.html
- **FastAPI**: https://fastapi.tiangolo.com/
- **OpenAI API**: https://platform.openai.com/docs
- **WebSocket**: https://developer.mozilla.org/en-US/docs/Web/API/WebSocket

### Datasets
- **WLASL**: http://www.wlasl.org/
- **ASL Bricks**: http://aslbricks.org/
- **ASL LEX**: https://asllex.org/

---

## ‚úÖ Deployment Status

### ‚úÖ Completed
- Backend pushed to Hugging Face Spaces
- Git LFS configured for large files (models, WLASL JSON)
- Ephemeral storage implemented (/tmp)
- Docker container optimized
- All 28 files deployed successfully

### ‚è≥ Pending
1. Set `OPENAI_API_KEY` in HF Spaces settings
2. Wait for Docker build (~10-15 minutes)
3. Update frontend `API_BASE_URL` to production URL
4. Deploy frontend to Vercel
5. Update Chrome extension backend URL
6. Publish extension to Chrome Web Store

---

## üìû Support & Documentation

- **API Documentation**: `/docs` endpoint (Swagger UI)
- **Health Check**: `/health` endpoint
- **Model Status**: `/model-status` endpoint
- **Logs**: HF Spaces build logs for debugging

---

**Last Updated**: October 31, 2025  
**Version**: 1.0.0  
**Status**: ‚úÖ Backend Deployed | ‚è≥ Frontend Pending
